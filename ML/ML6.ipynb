{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Tic-Tac-Toe board\n",
    "def display_board(board):\n",
    "    print(\"\\n\".join([\" | \".join(board[i*3:(i+1)*3]) for i in range(3)]))\n",
    "    print(\"-\" * 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there's a winner\n",
    "def check_winner(board, player):\n",
    "    wins = [(0,1,2), (3,4,5), (6,7,8), (0,3,6), (1,4,7), (2,5,8), (0,4,8), (2,4,6)]\n",
    "    return any(all(board[i] == player for i in win) for win in wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning agent functions\n",
    "q_table = {}  # Stores Q-values for state-action pairs\n",
    "def get_state(board):\n",
    "    return \"\".join(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(board, epsilon=0.1):\n",
    "    state = get_state(board)\n",
    "    if random.random() < epsilon or state not in q_table:\n",
    "        return random.choice([i for i, x in enumerate(board) if x == ' '])\n",
    "    return max((i for i, x in enumerate(board) if x == ' '), key=lambda x: q_table[state].get(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q(state, action, reward, next_state, alpha=0.5, gamma=0.9):\n",
    "    if state not in q_table:\n",
    "        q_table[state] = {}\n",
    "    old_q = q_table[state].get(action, 0)\n",
    "    next_max = max(q_table.get(next_state, {}).values(), default=0)\n",
    "    q_table[state][action] = old_q + alpha * (reward + gamma * next_max - old_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the AI through reinforcement learning\n",
    "def train_ai(episodes=5000):\n",
    "    for _ in range(episodes):\n",
    "        board = [' '] * 9\n",
    "        while True:\n",
    "            state = get_state(board)\n",
    "            action = choose_action(board)\n",
    "            board[action] = 'X'\n",
    "            if check_winner(board, 'X'):\n",
    "                update_q(state, action, 1, get_state(board))\n",
    "                break\n",
    "            if ' ' not in board:\n",
    "                update_q(state, action, 0.5, get_state(board))\n",
    "                break\n",
    "            opp_action = random.choice([i for i, x in enumerate(board) if x == ' '])\n",
    "            board[opp_action] = 'O'\n",
    "            if check_winner(board, 'O'):\n",
    "                update_q(state, action, -1, get_state(board))\n",
    "                break\n",
    "            update_q(state, action, 0, get_state(board))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a game against the trained AI\n",
    "def play_game():\n",
    "    board = [' '] * 9\n",
    "    while True:\n",
    "        display_board(board)\n",
    "        ai_action = choose_action(board, epsilon=0)  # No exploration in test\n",
    "        board[ai_action] = 'X'\n",
    "        print(\"\\nAI moved:\")\n",
    "        display_board(board)\n",
    "        if check_winner(board, 'X'):\n",
    "            print(\"AI wins!\")\n",
    "            break\n",
    "        if ' ' not in board:\n",
    "            print(\"It's a draw!\")\n",
    "            break\n",
    "        while True:\n",
    "            try:\n",
    "                player_action = int(input(\"\\nEnter your move (0-8): \"))\n",
    "                if board[player_action] == ' ':\n",
    "                    board[player_action] = 'O'\n",
    "                    break\n",
    "                print(\"Invalid move, try again.\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid input, try again.\")\n",
    "        if check_winner(board, 'O'):\n",
    "            display_board(board)\n",
    "            print(\"You win!\")\n",
    "            break\n",
    "        if ' ' not in board:\n",
    "            display_board(board)\n",
    "            print(\"It's a draw!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AI...\n",
      "Training complete!\n",
      "\n",
      "Game starts! You are 'O', AI is 'X'\n",
      "Positions (0-8):\n",
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "print(\"Training AI...\")\n",
    "train_ai()\n",
    "print(\"Training complete!\\n\")\n",
    "print(\"Game starts! You are 'O', AI is 'X'\")\n",
    "print(\"Positions (0-8):\")\n",
    "print(np.arange(9).reshape(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |   |  \n",
      "  |   |  \n",
      "  |   |  \n",
      "---------\n",
      "\n",
      "AI moved:\n",
      "  | X |  \n",
      "  |   |  \n",
      "  |   |  \n",
      "---------\n",
      "O | X |  \n",
      "  |   |  \n",
      "  |   |  \n",
      "---------\n",
      "\n",
      "AI moved:\n",
      "O | X |  \n",
      "  | X |  \n",
      "  |   |  \n",
      "---------\n",
      "O | X |  \n",
      "  | X | O\n",
      "  |   |  \n",
      "---------\n",
      "\n",
      "AI moved:\n",
      "O | X | X\n",
      "  | X | O\n",
      "  |   |  \n",
      "---------\n",
      "Invalid move, try again.\n",
      "O | X | X\n",
      "  | X | O\n",
      "O |   |  \n",
      "---------\n",
      "\n",
      "AI moved:\n",
      "O | X | X\n",
      "  | X | O\n",
      "O | X |  \n",
      "---------\n",
      "AI wins!\n"
     ]
    }
   ],
   "source": [
    "play_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "-------------\n",
    "\n",
    "We import numpy (for array manipulation, though it’s minimal here) and random for randomness in choosing moves, especially for exploration in the Q-learning phase.\n",
    "\n",
    "Display the Tic-Tac-Toe Board:\n",
    "This function displays the Tic-Tac-Toe board in a readable format. The board is a 1D list of 9 elements, and we use slicing to create rows.\n",
    "\n",
    "Check for a Winner:\n",
    "his function checks if a player has won. The variable wins defines all possible winning combinations, and the function checks if any of these contain the same player symbol.\n",
    "Suggestion: Add variations by allowing different board sizes or modifying winning conditions.\n",
    "\n",
    "Q-learning Setup:\n",
    "q_table is a dictionary that holds the Q-values for each board state-action pair, allowing the AI to learn optimal moves over time.\n",
    "\n",
    "Get State:\n",
    "Explanation: The board’s state is converted into a string so it can serve as a unique key for q_table.\n",
    "\n",
    "Choose Action:\n",
    "This function selects the AI’s action. With probability epsilon, it explores random moves; otherwise, it exploits the best-known move based on q_table.\n",
    "Suggestion: Change the epsilon parameter or decrease it gradually to experiment with the balance between exploration and exploitation.\n",
    "\n",
    "Update Q-values:\n",
    "This function updates Q-values based on the reward for the current action and the maximum expected reward from the next state, applying the Q-learning formula.\n",
    "Parameters:\n",
    "alpha (learning rate): Controls the weight given to new information.\n",
    "gamma (discount factor): Determines how much future rewards affect the current state.\n",
    "Suggestion: Adjust alpha and gamma values to see how quickly the AI learns or how it values future rewards over immediate ones.\n",
    "\n",
    "Q-learning Algorithm Explanation\n",
    "Q-learning is a reinforcement learning algorithm that aims to learn a policy by maximizing cumulative rewards. Here’s how it fits into this Tic-Tac-Toe game:\n",
    "\n",
    "Q-table: Stores Q-values that represent the desirability of actions given states.\n",
    "Exploration and Exploitation: The AI explores random moves at first (epsilon), then exploits learned moves as it builds up Q-values.\n",
    "Update Rule: Updates Q-values based on actions taken, rewards received, and maximum Q-values of resulting states to improve decision-making over time.\n",
    "\n",
    "Training the AI:\n",
    "This function trains the AI through episodes of self-play. The AI (X) and a random opponent (O) alternate turns. Rewards are given based on the outcome of each episode.\n",
    "Rewards:\n",
    "Win: +1\n",
    "Draw: 0.5\n",
    "Loss: -1\n",
    "Suggestion: Increase or decrease the number of episodes to control training depth. More episodes can lead to better AI performance.\n",
    "\n",
    "Play Against the AI:\n",
    "This function allows a player to play against the AI. It uses the trained q_table to guide AI moves, and disables exploration (epsilon=0) to ensure the AI follows its learned strategy.\n",
    "Suggestion: Play around with epsilon to see how random moves affect gameplay or modify the player symbols.\n",
    "\n",
    "Main Execution:\n",
    "This segment trains the AI, then initiates a game with the player.\n",
    "Suggestion: Add a prompt for the player to choose X or O and dynamically adjust the AI’s training reward structure for that symbol.\n",
    "\n",
    "Summary\n",
    "Q-learning is essential here for reinforcement learning in a static environment where actions lead to rewards or penalties, making it ideal for turn-based games like Tic-Tac-Toe.\n",
    "Parameter Tuning (alpha, gamma, epsilon, episodes): Adjusting these values allows control over the AI’s learning rate, exploration-exploitation balance, and emphasis on immediate vs. future rewards.\n",
    "By altering these aspects, you can observe different behaviors and outcomes, allowing deeper insight into how Q-learning influences the game dynamics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0b15402db86e3924a8be0f40e4477bbc39fc6773c61cc56d96ddc22ec1bec37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
