{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for the game\n",
    "EMPTY = 0\n",
    "X = 1\n",
    "O = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = [EMPTY] * 9  # 3x3 grid\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = [EMPTY] * 9\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        return self.board\n",
    "\n",
    "    def render(self):\n",
    "        for i in range(3):\n",
    "            print(self.board[i*3:i*3+3])\n",
    "        print()\n",
    "\n",
    "    def is_winner(self, player):\n",
    "        win_conditions = [\n",
    "            [0, 1, 2], [3, 4, 5], [6, 7, 8],  # rows\n",
    "            [0, 3, 6], [1, 4, 7], [2, 5, 8],  # columns\n",
    "            [0, 4, 8], [2, 4, 6]  # diagonals\n",
    "        ]\n",
    "        for condition in win_conditions:\n",
    "            if all(self.board[i] == player for i in condition):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def is_full(self):\n",
    "        return EMPTY not in self.board\n",
    "\n",
    "    def step(self, action, player):\n",
    "        if self.board[action] != EMPTY:\n",
    "            return self.board, -10, True  # Invalid move penalty\n",
    "\n",
    "        self.board[action] = player\n",
    "\n",
    "        if self.is_winner(player):\n",
    "            return self.board, 10, True  # Win reward\n",
    "\n",
    "        if self.is_full():\n",
    "            return self.board, 0, True  # Draw reward\n",
    "\n",
    "        return self.board, 0, False  # No winner yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.q_table = {}\n",
    "\n",
    "    def get_q(self, state, action):\n",
    "        state = tuple(state)\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = [0] * 9  # Initialize Q-values for all actions\n",
    "        return self.q_table[state][action]\n",
    "\n",
    "    def update_q(self, state, action, reward, next_state, done):\n",
    "        state = tuple(state)\n",
    "        next_state = tuple(next_state)\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = [0] * 9\n",
    "\n",
    "        best_next_action = max(self.q_table[next_state]) if not done else 0\n",
    "        q_value = self.get_q(state, action)\n",
    "        new_q_value = q_value + self.alpha * (reward + self.gamma * best_next_action - q_value)\n",
    "        self.q_table[state][action] = new_q_value\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice([i for i in range(9) if state[i] == EMPTY])  # Explore\n",
    "        q_values = [self.get_q(state, i) if state[i] == EMPTY else -float('inf') for i in range(9)]\n",
    "        return np.argmax(q_values)  # Exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, num_episodes=10000):\n",
    "    game = TicTacToe()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = game.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = game.step(action, X)  # X is the agent\n",
    "            agent.update_q(state, action, reward, next_state, done)\n",
    "            state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent, num_games=100):\n",
    "    game = TicTacToe()\n",
    "    agent_wins, opponent_wins, draws = 0, 0, 0\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        state = game.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = game.step(action, X)\n",
    "            if reward == 10:\n",
    "                agent_wins += 1\n",
    "            elif reward == -10:\n",
    "                opponent_wins += 1\n",
    "            state = next_state\n",
    "\n",
    "    print(f\"Agent wins: {agent_wins}, Opponent wins: {opponent_wins}, Draws: {draws}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the agent\n",
    "agent = QLearningAgent()\n",
    "train(agent, num_episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent wins: 100, Opponent wins: 0, Draws: 0\n"
     ]
    }
   ],
   "source": [
    "# Testing the agent\n",
    "test(agent, num_games=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the Code:\n",
    "\n",
    "a. Setting up the environment\n",
    "The TicTacToe class simulates the environment. It initializes the game state (board), checks for wins, handles game steps (actions), and displays the current state with render().\n",
    "\n",
    "b. Defining the Tic-Tac-Toe game\n",
    "The game is played on a 3x3 grid. The agent plays as X, and we check for winning conditions after each move. The game ends when there's a win or a draw.\n",
    "\n",
    "c. Building the reinforcement learning model\n",
    "The QLearningAgent class represents the Q-learning agent. The agent uses a Q-table (q_table) where each key is a game state, and the value is a list of Q-values for each possible action.\n",
    "The agent uses the epsilon-greedy strategy: it either explores random moves or exploits the best-known move (based on the Q-values).\n",
    "\n",
    "d. Training the model\n",
    "In the train function, the agent plays num_episodes games, learning from each game's rewards. After every action, the Q-table is updated using the Q-learning update rule:\n",
    "Q(st,at)=Q(st,at)+α(rt+γ⋅maxQ(st+1,a)−Q(st,at))\n",
    "where:\n",
    "α is the learning rate.\n",
    "γ is the discount factor.\n",
    "r_t is the reward from the action.\n",
    "\n",
    "e. Testing the model\n",
    "The test function simulates multiple games against a random opponent to evaluate how well the trained agent performs. It counts agent wins, opponent wins, and draws.\n",
    "\n",
    "*Main Algorithms Used:*\n",
    "Q-Learning: A reinforcement learning algorithm where an agent learns by interacting with the environment and adjusting its action choices based on past experiences.\n",
    "Epsilon-Greedy: A strategy that balances exploration and exploitation.\n",
    "Conclusion:\n",
    "This code sets up a simple Q-learning agent to play Tic-Tac-Toe. The model learns from repeated games, adjusts its strategies, and can be tested by playing against a random opponent to evaluate its performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of Changes:\n",
    "render() method: This method is called after each move in the step() function to print the current state of the game board. It displays the board in a user-friendly format with vertical bars (|) separating the columns and dashes (-) separating the rows.\n",
    "\n",
    "Example output for a game board:\n",
    "\n",
    "X | O |  \n",
    "\n",
    "-----\n",
    "O | X | O\n",
    "\n",
    "-----\n",
    "X |   |  \n",
    "\n",
    "Visual Output during Gameplay: After every move, self.render() is called to print the updated board, which allows you to track how the game evolves visually.\n",
    "\n",
    "Example of Game Play:\n",
    "Let's consider a random game scenario where the agent plays against an opponent. The output will look like this:\n",
    "\n",
    "X |   |  \n",
    "\n",
    "-----\n",
    "O | X |  \n",
    "\n",
    "-----\n",
    "   |   | O\n",
    "\n",
    "X | O |  \n",
    "\n",
    "-----\n",
    "O | X |  \n",
    "\n",
    "-----\n",
    "X | O |  \n",
    "\n",
    "This gives you a step-by-step visualization of how the game progresses, showing the state of the board after each move."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0b15402db86e3924a8be0f40e4477bbc39fc6773c61cc56d96ddc22ec1bec37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
